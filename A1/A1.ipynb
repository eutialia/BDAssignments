{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1: Web Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data scientists often need to crawl data from websites and turn the crawled data (HTML pages) to structured data (tables). Thus, web scraping is an essential skill that every data scientist should master. In this assignment, you will learn the followings:\n",
    "\n",
    "\n",
    "* How to download HTML pages from a website?\n",
    "* How to extract relevant content from an HTML page? \n",
    "\n",
    "Furthermore, you will gain a deeper understanding of the data science lifecycle.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. Please use [pandas.DataFrame](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html) rather than spark.DataFrame to manipulate data.\n",
    "\n",
    "2. Please use [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) rather than [lxml](http://lxml.de/) to parse an HTML page and extract data from the page.\n",
    "\n",
    "3. Please follow python code style (https://www.python.org/dev/peps/pep-0008/). If TA finds your code hard to read, you will lose points. This requirement will stay for the whole semester."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminary"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "If this is your first time to write a web scraper, you need to learn some basic knowledge of this topic. I found that this is a good resource: [Tutorial: Web Scraping and BeautifulSoup](https://www.dataquest.io/blog/web-scraping-beautifulsoup/). \n",
    "\n",
    "Please let me know if you find a better resource. I'll share it with the other students."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Imagine you are a data scientist working at SFU. Your job is to extract insights from SFU data to answer questions. \n",
    "\n",
    "In this assignment, you will do two tasks. Please recall the high-level data science lifecycle below. I suggest that when doing this assignment, please remind yourself of what data you collected and what questions you tried to answer.\n",
    "\n",
    "\n",
    "<img src=\"lifecycle.png\" width=\"500\">\n",
    "<center><h4>Figure 1. Data Science Lifecycle</h4></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 1: SFU CS Faculty Members"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sometimes you don't know what questions to ask. No worries. Start collecting data first. \n",
    "\n",
    "In Task 1, your job is to write a web scraper to extract the faculty information from this page: [https://www.sfu.ca/computing/people/faculty.html](https://www.sfu.ca/computing/people/faculty.html).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (a) Crawl Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A web page is essentially a file stored in a remote machine (called web server). Please write code to download the HTML page and save it as a text file (\"csfaculty.html\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "import requests\n",
    "\n",
    "URL = 'https://www.sfu.ca/computing/people/faculty.html'\n",
    "response = requests.get(URL)\n",
    "\n",
    "with open('csfaculty.html', 'w', encoding='utf-8') as html_file:\n",
    "    html_file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (b) Extract Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Please write code to extract relevant content (name, rank, area, profile, homepage) from \"csfaculty.html\" and save them as a CSV file (like [faculty_table.csv](./faculty_table.csv)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('csfaculty.html', 'r', encoding='utf-8') as html_file:\n",
    "    bs = BeautifulSoup(html_file, 'html.parser')\n",
    "relevant_content = bs.select('.textimage.section > div')\n",
    "\n",
    "def get_name_and_rank(element):\n",
    "    name_and_rank = element.select('div:nth-child(2) > h4:first-child')[0].text\n",
    "    split = name_and_rank.replace('\\n', '').split(', ')\n",
    "    return split[0], split[1]\n",
    "\n",
    "def get_area(element):\n",
    "    area = element.select('div:nth-child(2) > p:nth-child(2)')[0].text[6:].replace('\\n', '') if len(element.select('div:nth-child(2) > p:nth-child(2)')) > 0 else ''\n",
    "    return area\n",
    "\n",
    "def get_profile(element):\n",
    "    if element.select('div:nth-child(2) > p:last-child > a:nth-of-type(1)[href]'):\n",
    "        profile = element.select('div:nth-child(2) > p:last-child > a:nth-of-type(1)[href]')[0]['href']\n",
    "        profile = 'http://www.sfu.ca' + profile if profile.startswith('/computing') else profile\n",
    "    elif element.select('div:nth-child(2) > p:nth-of-type(2) > a:nth-of-type(1)[href]'):\n",
    "        profile = element.select('div:nth-child(2) > p:nth-of-type(2) > a:nth-of-type(1)[href]')[0]['href']\n",
    "        profile = 'http://www.sfu.ca' + profile if profile.startswith('/computing') else profile\n",
    "    else:\n",
    "        profile = ''\n",
    "    return profile\n",
    "\n",
    "def get_homepage(element):\n",
    "    if element.select('div:nth-child(2) > p:last-child > a:nth-of-type(2)[href]'):\n",
    "        homepage = element.select('div:nth-child(2) > p:last-child > a:nth-of-type(2)[href]')[0]['href']\n",
    "        homepage = 'http://www.sfu.ca' + homepage if homepage.startswith('/computing') else homepage\n",
    "    elif element.select('div:nth-child(2) > p:nth-of-type(2) > a:nth-of-type(2)[href]'):\n",
    "        homepage = element.select('div:nth-child(2) > p:nth-of-type(2) > a:nth-of-type(2)[href]')[0]['href']\n",
    "        'http://www.sfu.ca' + homepage if homepage.startswith('/computing') else homepage\n",
    "    else:\n",
    "        homepage = ''\n",
    "    return homepage\n",
    "\n",
    "names = []\n",
    "ranks = []\n",
    "areas = []\n",
    "profiles = []\n",
    "homepages = []\n",
    "\n",
    "for element in relevant_content:\n",
    "    names.append(get_name_and_rank(element)[0])\n",
    "    ranks.append(get_name_and_rank(element)[1])\n",
    "    areas.append(get_area(element))\n",
    "    profiles.append(get_profile(element))\n",
    "    homepages.append(get_homepage(element))\n",
    "\n",
    "extract = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'rank': ranks,\n",
    "    'area': areas,\n",
    "    'profile': profiles,\n",
    "    'homepage': homepages\n",
    "})\n",
    "\n",
    "extract.to_csv('faculty_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (c) Interesting Finding"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note that you don't need to do anything for Task 1(c). The purpose of this part is to give you some sense about how to leverage exploratory data analysis (EDA) to come up with interesting questions about the data. EDA is an important topic in data science; you will  learn it soon from this course. \n",
    "\n",
    "\n",
    "First, please install [dataprep](http://dataprep.ai).\n",
    "Then, run the cell below. \n",
    "It shows a bar chart for every column. What intersting findings can you get from these visualizations? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import plot\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"faculty_table.csv\")\n",
    "plot(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Below are some examples:\n",
    "\n",
    "**Finding 1:** Professor# (29) is more than 3x larger than Associate Professor# (9). \n",
    "\n",
    "**Questions:** Why did it happen? Is it common in all CS schools in Canada? Will the gap go larger or smaller in five years? What actions can be taken to enlarge/shrink the gap?\n",
    "\n",
    "\n",
    "**Finding 2:** Homepage has 20.3% missing values. \n",
    "\n",
    "**Questions:** Why are there so many missing values? Is it because many faculty do not have their own homepages or do not add their homepages to the school page? What actions can be taken to avoid this to happen in the future? "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Task 2: Age Follows Normal Distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "In this task, you start with a question and then figure out what data to collect.\n",
    "\n",
    "The question that you are intersted in is `Does SFU CS faculty age follow a normal distribution?`\n",
    "\n",
    "To estimate the age of a faculty member, you can collect the year in which s/he graduates from a university (`gradyear`) and then estimate `age` using the following equation:\n",
    "\n",
    "$$age \\approx 2020+23 - gradyear$$\n",
    "\n",
    "For example, if one graduates from a university in 1990, then the age is estimated as 2020+23-1990 = 53. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (a) Crawl Web Page"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "You notice that faculty profile pages contain graduation information. For example, you can see that Dr. Jiannan Wang graduated from Harbin Institute of Technology in 2008 at [http://www.sfu.ca/computing/people/faculty/jiannanwang.html](http://www.sfu.ca/computing/people/faculty/jiannanwang.html). \n",
    "\n",
    "\n",
    "Please write code to download the 64 profile pages and save each page as a text file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code\n",
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "\n",
    "with open('faculty_table.csv', 'r') as csv:\n",
    "    df = pd.read_csv(csv)\n",
    "os.mkdir('profiles')\n",
    "for name, homepage in zip(df['name'], df['profile']):\n",
    "    if homepage != homepage:\n",
    "        continue\n",
    "    response = requests.get(homepage)\n",
    "    with open('profiles/' + name.lower().replace(' ', '_') + '.html', 'w', encoding='utf-8') as html_file:\n",
    "        html_file.write(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### (b) Extract Structured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Please write code to extract the earliest graduation year (e.g., 2008 for Dr. Jiannan Wang) from each profile page, and create a csv file like [faculty_grad_year.csv](./faculty_grad_year.csv). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code here\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "names = []\n",
    "grad_years = []\n",
    "\n",
    "def get_grad_years(element):\n",
    "    parent_c0 = element.select('.parsys_column.cq-colctrl-lt0 > div:first-child')\n",
    "    parent_c1 = element.select('.parsys_column.cq-colctrl-lt0 > div:last-child')\n",
    "    if parent_c0[0].find('div', class_='title section'):\n",
    "        #pic on the right: ul\n",
    "        parent = parent_c0[0].select('div:nth-child(3) > div > ul > li:last-of-type')[0].text\n",
    "        text = parent.replace('.', '').replace('\\n', '').replace('\\xa0', '').rstrip().split(' ')\n",
    "        return text[-1] if text[-1].isdigit() else ''\n",
    "    else:\n",
    "        #pic on the left: p / ul\n",
    "        parent = parent_c1[0].select('div:first-child > div > p:first-of-type')\n",
    "        if parent:\n",
    "            text = parent[0].text\n",
    "        elif parent_c1[0].select('div:first-child > div > ul > li:last-of-type'):\n",
    "            text = parent_c1[0].select('div:first-child > div > ul > li:last-of-type')[0].text\n",
    "        else:\n",
    "            text = parent_c1[0].select('div:nth-child(2) > div > p:first-of-type')[0].text\n",
    "\n",
    "        text = text.replace('.', '').replace('\\n', '').replace('\\xa0', '').rstrip().split(' ')\n",
    "        return text[-1] if text[-1].isdigit() else ''\n",
    "\n",
    "for filename in sorted(os.listdir('profiles')):\n",
    "    with open('profiles/' + filename, 'r') as filename_:\n",
    "        bs = BeautifulSoup(filename_, 'html.parser')\n",
    "    names.append(bs.select('.title.section > div > h1')[0].text.split(',')[0])\n",
    "    grad_years.append(get_grad_years(bs))\n",
    "\n",
    "extract = pd.DataFrame({\n",
    "    'name': names,\n",
    "    'gradyear': grad_years\n",
    "})\n",
    "\n",
    "extract.to_csv('faculty_grad_year.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Similar to Task 1(c), you don't need to do anything here. Just look at different visualizations w.r.t. age and give yourself an answer to the question: `Does SFU CS faculty age follow a normal distribution?`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataprep.eda import plot\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"csfaculty_grad_year.csv\")\n",
    "df[\"age\"] = 2020+23-df[\"gradyear\"]\n",
    "\n",
    "plot(df, \"age\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Complete the code in this [notebook](https://github.com/sfu-db/bigdata-cmpt733/blob/master/Assignments/A1/A1.ipynb), and submit it to the CourSys activity `Assignment 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "<hr/>\n",
    "## <span style=\"color:red\">Bonus: Contribute to dataprep</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "**Total Bonus: 0.2 + 0.3 = 0.5**\n",
    "\n",
    "1. If you create an issue (i.e., report a bug or request a feature) at [link1](https://github.com/sfu-db/dataprep/issues) or [link2](https://github.com/sfu-db/DataConnectorConfigs/issues), you will get **0.2** bonus points.\n",
    "\n",
    "2.  Creating more issues will *not* give you more bonus points, but you are encouraged to do so for learning more.\n",
    "\n",
    "2. If you send a pull request (i.e., fix a bug or implement a feature or add data connector for a new website) at [link1](https://github.com/sfu-db/dataprep/pulls) or [link2](https://github.com/sfu-db/DataConnectorConfigs/pulls) and the pull request gets merged into the repo, you will get **0.3** bonus points. \n",
    "\n",
    "4.  Sending more pull requests will *not* give you more bonus points, but you are encouraged to do so for learning more.\n",
    "\n",
    "5. These bonus points will be directly added to your final grade.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**How to Submit**\n",
    "* Submit github link(s) to the CourSys activity `Bonus`.\n",
    "* Due on `March 15, 2020`\n",
    "\n",
    "If you love dataprep, please support it by simply clicking the **<span style=\"color:red\">Star</span>** on [Github](https://github.com/sfu-db/dataprep).\n",
    "<img src=\"dataprep-star.png\" width=\"1000\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}